{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bt978/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, LSTM, Reshape, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers  import TimeDistributed, TimeDistributedDense\n",
    "import os, sys, re\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.6 (default, Mar 11 2014, 23:42:44) \n",
      "[GCC Intel(R) C++ gcc 4.4 mode]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = './speech_data/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the labels \n",
    "df = pd.read_csv('./speech_data/presidents_meta.csv')\n",
    "label_dict = dict(zip(list(df.foldername), list(df.label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 862 texts.\n",
      "Found 862 texts.\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "# labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if 'combined' not in fname and 'combines' not in fname:\n",
    "                president_name = path.split('/')[-1]\n",
    "                fpath = os.path.join(path, fname)\n",
    "                f = open(fpath)\n",
    "                if (president_name in label_dict):\n",
    "                    labels.append(label_dict[president_name])\n",
    "                    line = f.read()\n",
    "                    line = re.sub(r'[^\\x00-\\x7F]+',' ', line).lower()\n",
    "                    texts.append(line)\n",
    "                f.close()\n",
    "                \n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "print('Found %s texts.' % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the president. good afternoon, ladies and gentlemen.i have been asked to give a statement about the consular convention that is pending before the united states senate.i should like to say very briefly that i hope the senate will give its advice and consent to the proposed convention with the u.s.s.r. i feel very strongly that the ratification of this treaty is very much in our national interest. i feel this way for two principal reasons:first, we need this treaty to protect 18,000 american citizens who each year travel from this country to the soviet the convention requires immediate notification to us whenever an american is arrested in the soviet union. it insures our right to visit that citizen within 4 and as often thereafter as is desirable.we think that we need these rights help to protect american citizens. these are rights which the soviet citizens already have who travel in this country, because guaranteed by our constitution.second, the convention does not require the openin'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[500][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862\n",
      "[2665, 11080, 10013, 1927, 81929, 62451, 51770, 54751, 5014, 6845]\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 100\n",
    "print(len(texts))\n",
    "mini_test = texts[:10]\n",
    "print(list(map(len, mini_test)))\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_SENTENCES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_dense_list(speech, maxsentence):\n",
    "    \n",
    "    each_sub_len = len(speech[0])\n",
    "    Z = np.zeros((maxsentence, each_sub_len))\n",
    "    #print(\"Shape of Z{}, list {}\", Z.shape, len(speech))\n",
    "    for index, row in enumerate(speech):\n",
    "        if (index >= maxsentence):\n",
    "            break\n",
    "        \n",
    "        Z[index] = row\n",
    "    #print(Z) \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_text(mini_test):\n",
    "    #mini_test = \"mini test. hello world.\"\n",
    "    tokenizer = Tokenizer(nb_words=5)\n",
    "    tokenizer.fit_on_texts(mini_test)\n",
    "    text_encoded = np.zeros((len(texts),MAX_SENTENCES,MAX_SEQUENCE_LENGTH))\n",
    "    #print(text_encoded.shape)\n",
    "    word_index = tokenizer.word_index\n",
    "    for enu, speech in enumerate(mini_test):\n",
    "        sents = tokenize.sent_tokenize(speech)\n",
    "        if(len(sents) == 0):\n",
    "            continue\n",
    "        sequences = tokenizer.texts_to_sequences(sents)\n",
    "        \n",
    "        #print(len(sequences))\n",
    "        #print('Found %s unique tokens.' % len(word_index))\n",
    "        data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        text_encoded[enu] = (pad_dense_list(data, 100))\n",
    "    return word_index, text_encoded\n",
    "word_index, text_encoded = preprocess_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(862, 100, 100)\n",
      "(862,)\n"
     ]
    }
   ],
   "source": [
    "# First Speech encoding\n",
    "data = text_encoded\n",
    "labels = np.asarray(labels)\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = data\n",
    "data = data.reshape(data.shape[0], data.shape[1]*data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(862, 10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (862, 10000)\n",
      "Shape of label tensor: (862,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "# labels = labels[indices]\n",
    "VALIDATION_SPLIT = 0.2\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "GLOVE_DIR = '../glove.6B/'\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(862, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_train = y_train.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NUM_SAMPLES = 32\n",
    "# VOCAB_SIZE = 1000\n",
    "# EMBEDDING_DIM = 64\n",
    "# MAX_SEQUENCE_LENGTH = 10\n",
    "# MAX_SENTENCES = 20\n",
    "# EMBEDDING_INPUT_LENGTH = MAX_SEQUENCE_LENGTH*MAX_SENTENCES\n",
    "# WORD_LSTM_DIM = 100\n",
    "# SENT_LSTM_DIM = 100\n",
    "# NUM_CLASSES = 7\n",
    "\n",
    "# input_array = np.random.randint(1000, size=(32, 20*10)) #batch = 32, sent = 20, words = 10\n",
    "# x_train = input_array[:25]\n",
    "# x_val = input_array[25:]\n",
    "# y_train = np.random.randint(1,7,25)\n",
    "# y_val = np.random.randint(1,7,7)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_val.shape)\n",
    "# print(y_val.shape)\n",
    "# embedding_matrix = np.random.randint(0,999,(1000,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_index) + 1\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_SENTENCES = 100\n",
    "EMBEDDING_INPUT_LENGTH = MAX_SEQUENCE_LENGTH*MAX_SENTENCES\n",
    "WORD_LSTM_DIM = 100\n",
    "SENT_LSTM_DIM = 100\n",
    "NUM_CLASSES = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical-Average model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 10000, 100)    0           embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 100, 100, 100) 0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 1, 100, 100)   0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 1, 100, 100)   80400       lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 100, 100)      0           timedistributed_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 100)           0           reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 100)           0           lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 7)             707         reshape_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 81107\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length=EMBEDDING_INPUT_LENGTH,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Reshape((MAX_SENTENCES, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM,), input_shape=(EMBEDDING_INPUT_LENGTH,EMBEDDING_DIM,)))\n",
    "# model.add(TimeDistributed(LSTM(WORD_LSTM_DIM, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), return_sequences=True)))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=2, keepdims=True), output_shape=lambda s: (s[0], 1, s[1],s[3])))\n",
    "model.add(TimeDistributed(LSTM(SENT_LSTM_DIM, input_shape=(MAX_SENTENCES, EMBEDDING_DIM), return_sequences=True)))\n",
    "model.add(Reshape((MAX_SENTENCES,SENT_LSTM_DIM), input_shape=(1,MAX_SENTENCES,SENT_LSTM_DIM)))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1, keepdims=True), output_shape=lambda s: (s[0], s[2])))\n",
    "model.add(Reshape((SENT_LSTM_DIM,)))\n",
    "model.add(Dense(NUM_CLASSES, input_dim=SENT_LSTM_DIM, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690, 10000)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 690 samples, validate on 172 samples\n",
      "Epoch 1/5\n",
      "690/690 [==============================] - 77s - loss: 1.6294 - acc: 0.2377 - val_loss: 2.1802 - val_acc: 0.2326\n",
      "Epoch 2/5\n",
      "690/690 [==============================] - 71s - loss: 1.5996 - acc: 0.2290 - val_loss: 2.1958 - val_acc: 0.1919\n",
      "Epoch 3/5\n",
      "690/690 [==============================] - 71s - loss: 1.5919 - acc: 0.2493 - val_loss: 2.2344 - val_acc: 0.2267\n",
      "Epoch 4/5\n",
      "690/690 [==============================] - 74s - loss: 1.5925 - acc: 0.2348 - val_loss: 2.2291 - val_acc: 0.0058\n",
      "Epoch 5/5\n",
      "690/690 [==============================] - 80s - loss: 1.5871 - acc: 0.2493 - val_loss: 2.1616 - val_acc: 0.2267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca73810>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),nb_epoch=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hierarchical-Sum model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 10000, 100)    0           embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)              (None, 100, 100, 100) 0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 1, 100, 100)   0           reshape_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_2 (TimeDistribute(None, 1, 100, 100)   80400       lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)              (None, 100, 100)      0           timedistributed_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 100)           0           reshape_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)              (None, 100)           0           lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 7)             707         reshape_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 81107\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length=EMBEDDING_INPUT_LENGTH,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Reshape((MAX_SENTENCES, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM,), input_shape=(EMBEDDING_INPUT_LENGTH,EMBEDDING_DIM,)))\n",
    "# model.add(TimeDistributed(LSTM(WORD_LSTM_DIM, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), return_sequences=True)))\n",
    "model.add(Lambda(lambda x: K.sum(x, axis=2, keepdims=True), output_shape=lambda s: (s[0], 1, s[1],s[3])))\n",
    "model.add(TimeDistributed(LSTM(SENT_LSTM_DIM, input_shape=(MAX_SENTENCES, EMBEDDING_DIM), return_sequences=True)))\n",
    "model.add(Reshape((MAX_SENTENCES,SENT_LSTM_DIM), input_shape=(1,MAX_SENTENCES,SENT_LSTM_DIM)))\n",
    "model.add(Lambda(lambda x: K.sum(x, axis=1, keepdims=True), output_shape=lambda s: (s[0], s[2])))\n",
    "model.add(Reshape((SENT_LSTM_DIM,)))\n",
    "model.add(Dense(NUM_CLASSES, input_dim=SENT_LSTM_DIM, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 10000, 100)    0           embedding_input_4[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "def Word_Sequential():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length=EMBEDDING_INPUT_LENGTH,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
