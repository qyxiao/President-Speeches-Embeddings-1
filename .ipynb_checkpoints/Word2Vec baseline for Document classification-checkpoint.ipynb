{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 19:04:40,274 : INFO : collecting all words and their counts\n",
      "2016-12-03 19:04:40,276 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-12-03 19:04:40,277 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2016-12-03 19:04:40,279 : INFO : Loading a fresh vocabulary\n",
      "2016-12-03 19:04:40,280 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2016-12-03 19:04:40,289 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2016-12-03 19:04:40,291 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2016-12-03 19:04:40,293 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2016-12-03 19:04:40,294 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2016-12-03 19:04:40,296 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2016-12-03 19:04:40,297 : INFO : resetting layer weights\n",
      "2016-12-03 19:04:40,299 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2016-12-03 19:04:40,301 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-03 19:04:40,313 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-03 19:04:40,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-03 19:04:40,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-03 19:04:40,318 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2016-12-03 19:04:40,320 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nDim = 400\n",
    "nwindow = 7\n",
    "minCount = 3\n",
    "nWorkers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = 'president_scraper/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for sub_dirName in os.listdir(self.dirname):\n",
    "            pathname = os.path.join(dirname, sub_dirName)\n",
    "            if(os.path.isdir(pathname)): #Check if file is director \n",
    "                for fname in os.listdir(pathname):\n",
    "                    for line in open(os.path.join(pathname, fname)):\n",
    "                        yield line.split()\n",
    "\n",
    "speeches = MySentences('president_scraper/data') # Parent directory of speeches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 19:05:25,875 : INFO : collecting all words and their counts\n",
      "2016-12-03 19:05:25,919 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-12-03 19:05:27,903 : INFO : PROGRESS: at sentence #10000, processed 1628396 words, keeping 47606 word types\n",
      "2016-12-03 19:05:28,491 : INFO : PROGRESS: at sentence #20000, processed 2042184 words, keeping 54810 word types\n",
      "2016-12-03 19:05:30,787 : INFO : PROGRESS: at sentence #30000, processed 3751279 words, keeping 77906 word types\n",
      "2016-12-03 19:05:32,445 : INFO : PROGRESS: at sentence #40000, processed 5120067 words, keeping 95093 word types\n",
      "2016-12-03 19:05:34,549 : INFO : PROGRESS: at sentence #50000, processed 6891797 words, keeping 115207 word types\n",
      "2016-12-03 19:05:35,116 : INFO : collected 118583 word types from a corpus of 7404211 raw words and 53373 sentences\n",
      "2016-12-03 19:05:35,117 : INFO : Loading a fresh vocabulary\n",
      "2016-12-03 19:05:35,522 : INFO : min_count=3 retains 52588 unique words (44% of original 118583, drops 65995)\n",
      "2016-12-03 19:05:35,523 : INFO : min_count=3 leaves 7272640 word corpus (98% of original 7404211, drops 131571)\n",
      "2016-12-03 19:05:35,833 : INFO : deleting the raw counts dictionary of 118583 items\n",
      "2016-12-03 19:05:35,840 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2016-12-03 19:05:35,842 : INFO : downsampling leaves estimated 5359899 word corpus (73.7% of prior 7272640)\n",
      "2016-12-03 19:05:35,843 : INFO : estimated required memory for 52588 words and 400 dimensions: 194575600 bytes\n",
      "2016-12-03 19:05:36,134 : INFO : resetting layer weights\n",
      "2016-12-03 19:05:37,872 : INFO : training model with 3 workers on 52588 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n",
      "2016-12-03 19:05:37,873 : INFO : expecting 53373 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-03 19:05:38,883 : INFO : PROGRESS: at 0.98% examples, 318026 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-03 19:05:39,895 : INFO : PROGRESS: at 2.42% examples, 330118 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:05:40,907 : INFO : PROGRESS: at 5.16% examples, 339005 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:05:41,927 : INFO : PROGRESS: at 8.30% examples, 339432 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:05:42,939 : INFO : PROGRESS: at 8.62% examples, 313376 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:05:43,959 : INFO : PROGRESS: at 10.11% examples, 315381 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:05:44,972 : INFO : PROGRESS: at 11.42% examples, 319935 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:05:45,978 : INFO : PROGRESS: at 12.76% examples, 322607 words/s, in_qsize 2, out_qsize 0\n",
      "2016-12-03 19:05:47,004 : INFO : PROGRESS: at 14.97% examples, 322725 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:05:48,014 : INFO : PROGRESS: at 16.29% examples, 322139 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:05:49,043 : INFO : PROGRESS: at 17.34% examples, 321539 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:05:50,049 : INFO : PROGRESS: at 19.35% examples, 324467 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:05:51,060 : INFO : PROGRESS: at 20.03% examples, 318111 words/s, in_qsize 6, out_qsize 1\n",
      "2016-12-03 19:05:52,071 : INFO : PROGRESS: at 20.95% examples, 316394 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:05:53,088 : INFO : PROGRESS: at 22.09% examples, 315312 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:05:54,094 : INFO : PROGRESS: at 23.52% examples, 315002 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:05:55,109 : INFO : PROGRESS: at 27.82% examples, 316408 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:05:56,114 : INFO : PROGRESS: at 28.68% examples, 317563 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:05:57,132 : INFO : PROGRESS: at 30.13% examples, 319397 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:05:58,148 : INFO : PROGRESS: at 31.45% examples, 321542 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:05:59,191 : INFO : PROGRESS: at 32.92% examples, 322639 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:00,208 : INFO : PROGRESS: at 35.44% examples, 323669 words/s, in_qsize 6, out_qsize 1\n",
      "2016-12-03 19:06:01,211 : INFO : PROGRESS: at 36.79% examples, 324701 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:02,224 : INFO : PROGRESS: at 37.92% examples, 322560 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:03,258 : INFO : PROGRESS: at 39.61% examples, 322886 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:04,281 : INFO : PROGRESS: at 40.46% examples, 323401 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:05,283 : INFO : PROGRESS: at 41.72% examples, 323603 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:06,299 : INFO : PROGRESS: at 43.33% examples, 323084 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:07,308 : INFO : PROGRESS: at 45.00% examples, 319216 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:08,423 : INFO : PROGRESS: at 47.83% examples, 315770 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:09,448 : INFO : PROGRESS: at 48.40% examples, 311612 words/s, in_qsize 6, out_qsize 2\n",
      "2016-12-03 19:06:10,461 : INFO : PROGRESS: at 49.84% examples, 311976 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:11,463 : INFO : PROGRESS: at 50.78% examples, 312351 words/s, in_qsize 1, out_qsize 0\n",
      "2016-12-03 19:06:12,492 : INFO : PROGRESS: at 52.61% examples, 312998 words/s, in_qsize 4, out_qsize 1\n",
      "2016-12-03 19:06:13,514 : INFO : PROGRESS: at 53.27% examples, 313801 words/s, in_qsize 4, out_qsize 0\n",
      "2016-12-03 19:06:14,515 : INFO : PROGRESS: at 56.02% examples, 314699 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-03 19:06:15,580 : INFO : PROGRESS: at 57.33% examples, 314825 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:16,613 : INFO : PROGRESS: at 58.05% examples, 313518 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:17,650 : INFO : PROGRESS: at 59.84% examples, 313890 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:18,655 : INFO : PROGRESS: at 60.83% examples, 315191 words/s, in_qsize 3, out_qsize 0\n",
      "2016-12-03 19:06:19,663 : INFO : PROGRESS: at 62.29% examples, 316073 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:20,676 : INFO : PROGRESS: at 63.50% examples, 313955 words/s, in_qsize 3, out_qsize 0\n",
      "2016-12-03 19:06:21,689 : INFO : PROGRESS: at 67.33% examples, 314412 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:22,696 : INFO : PROGRESS: at 68.52% examples, 315134 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:23,715 : INFO : PROGRESS: at 70.11% examples, 315937 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:24,717 : INFO : PROGRESS: at 71.42% examples, 316903 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:25,725 : INFO : PROGRESS: at 72.81% examples, 317702 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:26,741 : INFO : PROGRESS: at 75.22% examples, 318736 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:27,752 : INFO : PROGRESS: at 76.75% examples, 319572 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:28,819 : INFO : PROGRESS: at 78.01% examples, 319971 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:29,843 : INFO : PROGRESS: at 79.81% examples, 320629 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:30,852 : INFO : PROGRESS: at 80.70% examples, 321101 words/s, in_qsize 5, out_qsize 1\n",
      "2016-12-03 19:06:31,880 : INFO : PROGRESS: at 82.09% examples, 321489 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:32,928 : INFO : PROGRESS: at 83.50% examples, 320156 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:33,940 : INFO : PROGRESS: at 87.32% examples, 320428 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:34,944 : INFO : PROGRESS: at 88.57% examples, 321136 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:35,956 : INFO : PROGRESS: at 90.13% examples, 321809 words/s, in_qsize 6, out_qsize 1\n",
      "2016-12-03 19:06:36,963 : INFO : PROGRESS: at 90.81% examples, 319428 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:37,963 : INFO : PROGRESS: at 92.21% examples, 318633 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:38,966 : INFO : PROGRESS: at 92.94% examples, 318965 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:39,970 : INFO : PROGRESS: at 95.64% examples, 319521 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:41,006 : INFO : PROGRESS: at 97.08% examples, 319807 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:42,016 : INFO : PROGRESS: at 98.05% examples, 319947 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-03 19:06:43,038 : INFO : PROGRESS: at 99.33% examples, 317612 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-03 19:06:44,067 : INFO : PROGRESS: at 99.99% examples, 316133 words/s, in_qsize 2, out_qsize 2\n",
      "2016-12-03 19:06:44,070 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-03 19:06:44,078 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-03 19:06:44,098 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-03 19:06:44,099 : INFO : training on 37021055 raw words (20940934 effective words) took 66.2s, 316220 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(speeches, size=nDim, window=nwindow, min_count=minCount, workers=nWorkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 19:20:03,925 : INFO : saving Word2Vec object under ../mymodel, separately None\n",
      "2016-12-03 19:20:03,928 : INFO : storing numpy array 'syn1neg' to ../mymodel.syn1neg.npy\n",
      "2016-12-03 19:20:07,360 : INFO : not storing attribute syn0norm\n",
      "2016-12-03 19:20:07,362 : INFO : storing numpy array 'syn0' to ../mymodel.syn0.npy\n",
      "2016-12-03 19:20:10,368 : INFO : not storing attribute cum_table\n",
      "2016-12-03 19:20:11,201 : INFO : saved ../mymodel\n",
      "2016-12-03 19:20:11,202 : INFO : loading Word2Vec object from ../mymodel\n",
      "2016-12-03 19:20:11,573 : INFO : loading syn1neg from ../mymodel.syn1neg.npy with mmap=None\n",
      "2016-12-03 19:20:14,109 : INFO : loading syn0 from ../mymodel.syn0.npy with mmap=None\n",
      "2016-12-03 19:20:15,355 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-12-03 19:20:15,356 : INFO : setting ignored attribute cum_table to None\n",
      "2016-12-03 19:20:15,356 : INFO : loaded ../mymodel\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('../mymodel')\n",
    "model = gensim.models.Word2Vec.load('../mymodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.accuracy('../questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string, re\n",
    "def normalize_corpus(text):\n",
    "    import string \n",
    "    for char in string.punctuation:\n",
    "        text = text.replace(char, ' ')#text.replace(char, ' ' + char + ' ')\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text) #remove non-ASCII chars\n",
    "    text = re.sub( '\\s+', ' ', text).lstrip().rstrip() #remove extra and trailing spaces\n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "president_embedding = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adams\n",
      "arthur\n",
      "bharrison\n",
      "buchanan\n",
      "bush\n",
      "carter\n",
      "cleveland\n",
      "clinton\n",
      "coolidge\n",
      "eisenhower\n",
      "fdroosevelt\n",
      "fillmore\n",
      "ford\n",
      "garfield\n",
      "grant\n",
      "gwbush\n",
      "harding\n",
      "harrison\n",
      "hayes\n",
      "hoover\n",
      "jackson\n",
      "jefferson\n",
      "johnson\n",
      "jqadams\n",
      "kennedy\n",
      "lbjohnson\n",
      "lincoln\n",
      "madison\n",
      "mckinley\n",
      "monroe\n",
      "nixon\n",
      "obama\n",
      "pierce\n",
      "polk\n",
      "reagan\n",
      "roosevelt\n",
      "taft\n",
      "taylor\n",
      "truman\n",
      "tyler\n",
      "vanburen\n",
      "washington\n",
      "wilson\n"
     ]
    }
   ],
   "source": [
    "#Save embedding for each president \n",
    "for sub_dirName in os.listdir(dataPath):\n",
    "        pathname = os.path.join(dirname, sub_dirName)\n",
    "        if(os.path.isdir(pathname)): #Check if file is director \n",
    "            president_name = pathname.split('/')[-1]\n",
    "            print(president_name)\n",
    "            embedding = np.zeros(nDim)\n",
    "            num_words = 0\n",
    "            for line in open(os.path.join(pathname, 'combined.txt')):\n",
    "                text = normalize_corpus(line)\n",
    "                num_words = num_words + len(text.split())\n",
    "                for word in text.split():\n",
    "                    if(word in model.vocab):\n",
    "                        word_embed = model[word]\n",
    "                        embedding = embedding + word_embed\n",
    "            #Save the embedding in a file\n",
    "            embedding = embedding.tolist()\n",
    "            embedding  = map(lambda x: x / num_words, embedding)\n",
    "            president_embedding[president_name] = embedding\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.983575228979\n",
      "0.977117426784\n"
     ]
    }
   ],
   "source": [
    "norm_Lincoln = numpy.linalg.norm(president_embedding['lincoln'])\n",
    "norm_Jackson = numpy.linalg.norm(president_embedding['jackson'])\n",
    "norm_Roosevelt = numpy.linalg.norm(president_embedding['roosevelt'])\n",
    "LINCOLN_embed = president_embedding['lincoln']\n",
    "ROOSEVELT_embed = president_embedding['roosevelt']\n",
    "JACKSON_embed = president_embedding['jackson']\n",
    "print(np.dot(LINCOLN_embed,ROOSEVELT_embed )/(norm_Lincoln * norm_Roosevelt ))\n",
    "print(np.dot(LINCOLN_embed,JACKSON_embed )/(norm_Lincoln * norm_Jackson ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as president_scraper/embeddings/wilson-embedding.txt\n"
     ]
    }
   ],
   "source": [
    "pickle.dump( president_embedding, open(\"../president_embeddings400.p\", \"wb\" ) )\n",
    "\n",
    "#embed_file_name =  'president_scraper/embeddings/'+ president_name + '-embedding.txt'\n",
    "#numpy.savetxt(embed_file_name, embedding, fmt='%.18e', delimiter=',')\n",
    "print('File saved as {}'.format(embed_file_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
