{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sivagurukannan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, LSTM, Reshape, Lambda, Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.layers  import TimeDistributed, TimeDistributedDense\n",
    "import os, sys, re\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "from keras.engine.topology import Merge, merge\n",
    "from IPython.display import SVG, display\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "nltk.download('punkt')\n",
    "import string \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.12 |Anaconda custom (x86_64)| (default, Jul  2 2016, 17:43:17) \n",
      "[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = './speech_data/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the labels \n",
    "df = pd.read_csv('./speech_data/presidents_meta.csv')\n",
    "label_dict = dict(zip(list(df.foldername), list(df.label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 862 texts.\n",
      "Found 862 texts.\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "# labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if 'combined' not in fname and 'combines' not in fname:\n",
    "                president_name = path.split('/')[-1]\n",
    "                fpath = os.path.join(path, fname)\n",
    "                f = open(fpath)\n",
    "                if (president_name in label_dict):\n",
    "                    labels.append(label_dict[president_name])\n",
    "                    line = f.read()\n",
    "                    line = re.sub(r'[^\\x00-\\x7F]+',' ', line).lower()\n",
    "                    texts.append(line)\n",
    "                f.close()\n",
    "                \n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "print('Found %s texts.' % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862\n",
      "[2665, 11080, 10013, 1927, 81929, 62451, 51770, 54751, 5014, 6845]\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 50\n",
    "print(len(texts))\n",
    "mini_test = texts[:10]\n",
    "print(list(map(len, mini_test)))\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "MAX_SENTENCES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_dense_list(speech, maxsentence):\n",
    "    each_sub_len = len(speech[0])\n",
    "    Z = np.zeros((maxsentence, each_sub_len))\n",
    "    #print(\"Shape of Z{}, list {}\", Z.shape, len(speech))\n",
    "    for index, row in enumerate(speech):\n",
    "        if (index >= maxsentence):\n",
    "            break\n",
    "        \n",
    "        Z[index] = row\n",
    "    #print(Z) \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_text(mini_test):\n",
    "    #mini_test = \"mini test. hello world.\"\n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(mini_test)\n",
    "    text_encoded = np.zeros((len(texts),MAX_SENTENCES,MAX_SEQUENCE_LENGTH))\n",
    "    #print(text_encoded.shape)\n",
    "    word_index = tokenizer.word_index\n",
    "    for enu, speech in enumerate(mini_test):\n",
    "        sents = tokenize.sent_tokenize(speech)\n",
    "        if(len(sents) == 0):\n",
    "            continue\n",
    "        sequences = tokenizer.texts_to_sequences(sents)\n",
    "        data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        text_encoded[enu] = (pad_dense_list(data, 50))\n",
    "    return word_index, text_encoded\n",
    "word_index, text_encoded = preprocess_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First Speech encoding\n",
    "data = text_encoded\n",
    "labels = np.asarray(labels)\n",
    "one_hot_labels = np.zeros((len(labels), 2))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    onehot = np.zeros(2)\n",
    "    if (labels[i] >= 4):\n",
    "        onehot[1] = 1\n",
    "    else :\n",
    "        onehot[0] = 1\n",
    "    one_hot_labels[i] = onehot\n",
    "labels = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = data\n",
    "data = data.reshape(data.shape[0], data.shape[1]*data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (862, 2500)\n",
      "Shape of label tensor: (862, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "VALIDATION_SPLIT = 0.2\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "GLOVE_DIR = '../glove.6B/'\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_index) + 1\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "MAX_SENTENCES = 50\n",
    "EMBEDDING_INPUT_LENGTH = MAX_SEQUENCE_LENGTH*MAX_SENTENCES\n",
    "WORD_LSTM_DIM = 100\n",
    "SENT_LSTM_DIM = 100\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical-Average model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length=EMBEDDING_INPUT_LENGTH,\n",
    "                            trainable=False, name = \"Embedding\")\n",
    "model.add(embedding_layer)\n",
    "model.add(Reshape((MAX_SENTENCES, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM,), input_shape=(EMBEDDING_INPUT_LENGTH,EMBEDDING_DIM,)))\n",
    "#model.add(TimeDistributed(LSTM(WORD_LSTM_DIM, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), return_sequences=True, name = 'Word-LSTM')))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=2, keepdims=True), output_shape=lambda s: (s[0], 1, s[1],s[3])))\n",
    "model.add(TimeDistributed(\n",
    "            LSTM(SENT_LSTM_DIM, input_shape=(MAX_SENTENCES, EMBEDDING_DIM), \n",
    "                 return_sequences=True, name = 'Sentence-LSTM')))\n",
    "model.add(Reshape((MAX_SENTENCES,SENT_LSTM_DIM), input_shape=(1,MAX_SENTENCES,SENT_LSTM_DIM)))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1, keepdims=True), output_shape=lambda s: (s[0], s[2])))\n",
    "model.add(Reshape((SENT_LSTM_DIM,)))\n",
    "model.add(Dense(NUM_CLASSES, input_dim=SENT_LSTM_DIM, activation='softmax'))\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.fit(x_train, y_train, validation_data=(x_val, y_val),nb_epoch=15, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=16)\n",
    "print(score)\n",
    "preds = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hierarchical-Sum model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length=EMBEDDING_INPUT_LENGTH,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Reshape((MAX_SENTENCES, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM,), input_shape=(EMBEDDING_INPUT_LENGTH,EMBEDDING_DIM,)))\n",
    "#model.add(TimeDistributed(LSTM(WORD_LSTM_DIM, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), return_sequences=True)))\n",
    "model.add(Lambda(lambda x: K.sum(x, axis=2, keepdims=True), output_shape=lambda s: (s[0], 1, s[1],s[3])))\n",
    "model.add(TimeDistributed(LSTM(SENT_LSTM_DIM, input_shape=(MAX_SENTENCES, EMBEDDING_DIM), return_sequences=True)))\n",
    "model.add(Reshape((MAX_SENTENCES,SENT_LSTM_DIM), input_shape=(1,MAX_SENTENCES,SENT_LSTM_DIM)))\n",
    "model.add(Lambda(lambda x: K.sum(x, axis=1, keepdims=True), output_shape=lambda s: (s[0], s[2])))\n",
    "model.add(Reshape((SENT_LSTM_DIM,)))\n",
    "model.add(Dense(NUM_CLASSES, input_dim=SENT_LSTM_DIM, activation='softmax'))\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['acc'])\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),nb_epoch=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical MAX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length=EMBEDDING_INPUT_LENGTH,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Reshape((MAX_SENTENCES, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM,), input_shape=(EMBEDDING_INPUT_LENGTH,EMBEDDING_DIM,)))\n",
    "#model.add(TimeDistributed(LSTM(WORD_LSTM_DIM, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), return_sequences=True)))\n",
    "model.add(Lambda(lambda x: K.max(x, axis=2, keepdims=True), output_shape=lambda s: (s[0], 1, s[1],s[3])))\n",
    "model.add(TimeDistributed(LSTM(SENT_LSTM_DIM, input_shape=(MAX_SENTENCES, EMBEDDING_DIM), return_sequences=True)))\n",
    "model.add(Reshape((MAX_SENTENCES,SENT_LSTM_DIM), input_shape=(1,MAX_SENTENCES,SENT_LSTM_DIM)))\n",
    "model.add(Lambda(lambda x: K.max(x, axis=1, keepdims=True), output_shape=lambda s: (s[0], s[2])))\n",
    "model.add(Reshape((SENT_LSTM_DIM,)))\n",
    "model.add(Dense(NUM_CLASSES, input_dim=SENT_LSTM_DIM, activation='softmax'))\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['acc'])\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),nb_epoch=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_index) + 1\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_WORD_LENGTH = 5\n",
    "MAX_SENTENCES = 5\n",
    "EMBEDDING_INPUT_LENGTH = MAX_WORD_LENGTH*MAX_SENTENCES\n",
    "WORD_LSTM_DIM = 100\n",
    "SENT_LSTM_DIM = 100\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_INPUT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(690, 2500)\n",
      "(690, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 25)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 25, 50)        1868250     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 1250)          0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)               (None, 250)           0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)               (None, 250)           0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)               (None, 250)           0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)               (None, 250)           0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_55 (Lambda)               (None, 250)           0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_51 (Reshape)             (None, 5, 50)         0           lambda_51[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_52 (Reshape)             (None, 5, 50)         0           lambda_52[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_53 (Reshape)             (None, 5, 50)         0           lambda_53[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_54 (Reshape)             (None, 5, 50)         0           lambda_54[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_55 (Reshape)             (None, 5, 50)         0           lambda_55[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lstm_51 (LSTM)                   (None, 128)           91648       reshape_51[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "lstm_52 (LSTM)                   (None, 128)           91648       reshape_52[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "lstm_53 (LSTM)                   (None, 128)           91648       reshape_53[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "lstm_54 (LSTM)                   (None, 128)           91648       reshape_54[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "lstm_55 (LSTM)                   (None, 128)           91648       reshape_55[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 640)           0           lstm_51[0][0]                    \n",
      "                                                                   lstm_52[0][0]                    \n",
      "                                                                   lstm_53[0][0]                    \n",
      "                                                                   lstm_54[0][0]                    \n",
      "                                                                   lstm_55[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             1282        merge_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2327772\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 754.57 556.00\" width=\"755pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-552 750.569,-552 750.569,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 6785104656 -->\n",
       "<g class=\"node\" id=\"node1\"><title>6785104656</title>\n",
       "<polygon fill=\"none\" points=\"309.104,-511.5 309.104,-547.5 437.466,-547.5 437.466,-511.5 309.104,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.285\" y=\"-525.3\">input_2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 6785104720 -->\n",
       "<g class=\"node\" id=\"node2\"><title>6785104720</title>\n",
       "<polygon fill=\"none\" points=\"291.214,-438.5 291.214,-474.5 455.355,-474.5 455.355,-438.5 291.214,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.285\" y=\"-452.3\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 6785104656&#45;&gt;6785104720 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>6785104656-&gt;6785104720</title>\n",
       "<path d=\"M373.285,-511.313C373.285,-503.289 373.285,-493.547 373.285,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"376.785,-484.529 373.285,-474.529 369.785,-484.529 376.785,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6341687824 -->\n",
       "<g class=\"node\" id=\"node3\"><title>6341687824</title>\n",
       "<polygon fill=\"none\" points=\"317.652,-365.5 317.652,-401.5 428.917,-401.5 428.917,-365.5 317.652,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.285\" y=\"-379.3\">flatten_2: Flatten</text>\n",
       "</g>\n",
       "<!-- 6785104720&#45;&gt;6341687824 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>6785104720-&gt;6341687824</title>\n",
       "<path d=\"M373.285,-438.313C373.285,-430.289 373.285,-420.547 373.285,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"376.785,-411.529 373.285,-401.529 369.785,-411.529 376.785,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6785105424 -->\n",
       "<g class=\"node\" id=\"node4\"><title>6785105424</title>\n",
       "<polygon fill=\"none\" points=\"3.55176,-292.5 3.55176,-328.5 135.018,-328.5 135.018,-292.5 3.55176,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69.2847\" y=\"-306.3\">lambda_51: Lambda</text>\n",
       "</g>\n",
       "<!-- 6341687824&#45;&gt;6785105424 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>6341687824-&gt;6785105424</title>\n",
       "<path d=\"M317.526,-369.478C269.116,-358.171 198.581,-341.697 145.058,-329.197\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"145.806,-325.778 135.272,-326.911 144.213,-332.594 145.806,-325.778\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5882140624 -->\n",
       "<g class=\"node\" id=\"node5\"><title>5882140624</title>\n",
       "<polygon fill=\"none\" points=\"156.552,-292.5 156.552,-328.5 288.018,-328.5 288.018,-292.5 156.552,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.285\" y=\"-306.3\">lambda_52: Lambda</text>\n",
       "</g>\n",
       "<!-- 6341687824&#45;&gt;5882140624 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>6341687824-&gt;5882140624</title>\n",
       "<path d=\"M337.116,-365.494C316.351,-355.73 290.078,-343.376 267.905,-332.951\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"269.22,-329.701 258.681,-328.614 266.241,-336.036 269.22,-329.701\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6363026064 -->\n",
       "<g class=\"node\" id=\"node6\"><title>6363026064</title>\n",
       "<polygon fill=\"none\" points=\"307.552,-292.5 307.552,-328.5 439.018,-328.5 439.018,-292.5 307.552,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.285\" y=\"-306.3\">lambda_53: Lambda</text>\n",
       "</g>\n",
       "<!-- 6341687824&#45;&gt;6363026064 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>6341687824-&gt;6363026064</title>\n",
       "<path d=\"M373.285,-365.313C373.285,-357.289 373.285,-347.547 373.285,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"376.785,-338.529 373.285,-328.529 369.785,-338.529 376.785,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6592871632 -->\n",
       "<g class=\"node\" id=\"node7\"><title>6592871632</title>\n",
       "<polygon fill=\"none\" points=\"458.552,-292.5 458.552,-328.5 590.018,-328.5 590.018,-292.5 458.552,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524.285\" y=\"-306.3\">lambda_54: Lambda</text>\n",
       "</g>\n",
       "<!-- 6341687824&#45;&gt;6592871632 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>6341687824-&gt;6592871632</title>\n",
       "<path d=\"M409.453,-365.494C430.218,-355.73 456.491,-343.376 478.664,-332.951\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"480.328,-336.036 487.888,-328.614 477.349,-329.701 480.328,-336.036\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6644038672 -->\n",
       "<g class=\"node\" id=\"node8\"><title>6644038672</title>\n",
       "<polygon fill=\"none\" points=\"611.552,-292.5 611.552,-328.5 743.018,-328.5 743.018,-292.5 611.552,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"677.285\" y=\"-306.3\">lambda_55: Lambda</text>\n",
       "</g>\n",
       "<!-- 6341687824&#45;&gt;6644038672 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>6341687824-&gt;6644038672</title>\n",
       "<path d=\"M429.043,-369.478C477.453,-358.171 547.989,-341.697 601.511,-329.197\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"602.356,-332.594 611.298,-326.911 600.764,-325.778 602.356,-332.594\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6572311824 -->\n",
       "<g class=\"node\" id=\"node9\"><title>6572311824</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 134.569,-255.5 134.569,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67.2847\" y=\"-233.3\">reshape_51: Reshape</text>\n",
       "</g>\n",
       "<!-- 6785105424&#45;&gt;6572311824 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>6785105424-&gt;6572311824</title>\n",
       "<path d=\"M68.8005,-292.313C68.5745,-284.289 68.3001,-274.547 68.0472,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"71.5446,-265.426 67.7644,-255.529 64.5474,-265.623 71.5446,-265.426\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6362306640 -->\n",
       "<g class=\"node\" id=\"node10\"><title>6362306640</title>\n",
       "<polygon fill=\"none\" points=\"153,-219.5 153,-255.5 287.569,-255.5 287.569,-219.5 153,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.285\" y=\"-233.3\">reshape_52: Reshape</text>\n",
       "</g>\n",
       "<!-- 5882140624&#45;&gt;6362306640 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>5882140624-&gt;6362306640</title>\n",
       "<path d=\"M221.801,-292.313C221.575,-284.289 221.3,-274.547 221.047,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.545,-265.426 220.764,-255.529 217.547,-265.623 224.545,-265.426\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6576213648 -->\n",
       "<g class=\"node\" id=\"node11\"><title>6576213648</title>\n",
       "<polygon fill=\"none\" points=\"306,-219.5 306,-255.5 440.569,-255.5 440.569,-219.5 306,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.285\" y=\"-233.3\">reshape_53: Reshape</text>\n",
       "</g>\n",
       "<!-- 6363026064&#45;&gt;6576213648 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>6363026064-&gt;6576213648</title>\n",
       "<path d=\"M373.285,-292.313C373.285,-284.289 373.285,-274.547 373.285,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"376.785,-265.529 373.285,-255.529 369.785,-265.529 376.785,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6593828944 -->\n",
       "<g class=\"node\" id=\"node12\"><title>6593828944</title>\n",
       "<polygon fill=\"none\" points=\"459,-219.5 459,-255.5 593.569,-255.5 593.569,-219.5 459,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"526.285\" y=\"-233.3\">reshape_54: Reshape</text>\n",
       "</g>\n",
       "<!-- 6592871632&#45;&gt;6593828944 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>6592871632-&gt;6593828944</title>\n",
       "<path d=\"M524.769,-292.313C524.995,-284.289 525.269,-274.547 525.522,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"529.022,-265.623 525.805,-255.529 522.025,-265.426 529.022,-265.623\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6645161872 -->\n",
       "<g class=\"node\" id=\"node13\"><title>6645161872</title>\n",
       "<polygon fill=\"none\" points=\"612,-219.5 612,-255.5 746.569,-255.5 746.569,-219.5 612,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"679.285\" y=\"-233.3\">reshape_55: Reshape</text>\n",
       "</g>\n",
       "<!-- 6644038672&#45;&gt;6645161872 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>6644038672-&gt;6645161872</title>\n",
       "<path d=\"M677.769,-292.313C677.995,-284.289 678.269,-274.547 678.522,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"682.022,-265.623 678.805,-255.529 675.025,-265.426 682.022,-265.623\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6572314384 -->\n",
       "<g class=\"node\" id=\"node14\"><title>6572314384</title>\n",
       "<polygon fill=\"none\" points=\"29.3623,-146.5 29.3623,-182.5 135.207,-182.5 135.207,-146.5 29.3623,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.2847\" y=\"-160.3\">lstm_51: LSTM</text>\n",
       "</g>\n",
       "<!-- 6572311824&#45;&gt;6572314384 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>6572311824-&gt;6572314384</title>\n",
       "<path d=\"M70.9157,-219.313C72.6108,-211.289 74.669,-201.547 76.5658,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"80.0443,-193.036 78.687,-182.529 73.1955,-191.589 80.0443,-193.036\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6361963408 -->\n",
       "<g class=\"node\" id=\"node15\"><title>6361963408</title>\n",
       "<polygon fill=\"none\" points=\"182.362,-146.5 182.362,-182.5 288.207,-182.5 288.207,-146.5 182.362,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235.285\" y=\"-160.3\">lstm_52: LSTM</text>\n",
       "</g>\n",
       "<!-- 6362306640&#45;&gt;6361963408 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>6362306640-&gt;6361963408</title>\n",
       "<path d=\"M223.916,-219.313C225.611,-211.289 227.669,-201.547 229.566,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"233.044,-193.036 231.687,-182.529 226.195,-191.589 233.044,-193.036\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6576210064 -->\n",
       "<g class=\"node\" id=\"node16\"><title>6576210064</title>\n",
       "<polygon fill=\"none\" points=\"320.362,-146.5 320.362,-182.5 426.207,-182.5 426.207,-146.5 320.362,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.285\" y=\"-160.3\">lstm_53: LSTM</text>\n",
       "</g>\n",
       "<!-- 6576213648&#45;&gt;6576210064 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>6576213648-&gt;6576210064</title>\n",
       "<path d=\"M373.285,-219.313C373.285,-211.289 373.285,-201.547 373.285,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"376.785,-192.529 373.285,-182.529 369.785,-192.529 376.785,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6593489808 -->\n",
       "<g class=\"node\" id=\"node17\"><title>6593489808</title>\n",
       "<polygon fill=\"none\" points=\"458.362,-146.5 458.362,-182.5 564.207,-182.5 564.207,-146.5 458.362,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"511.285\" y=\"-160.3\">lstm_54: LSTM</text>\n",
       "</g>\n",
       "<!-- 6593828944&#45;&gt;6593489808 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>6593828944-&gt;6593489808</title>\n",
       "<path d=\"M522.654,-219.313C520.959,-211.289 518.9,-201.547 517.004,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520.374,-191.589 514.882,-182.529 513.525,-193.036 520.374,-191.589\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6645158096 -->\n",
       "<g class=\"node\" id=\"node18\"><title>6645158096</title>\n",
       "<polygon fill=\"none\" points=\"604.362,-146.5 604.362,-182.5 710.207,-182.5 710.207,-146.5 604.362,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"657.285\" y=\"-160.3\">lstm_55: LSTM</text>\n",
       "</g>\n",
       "<!-- 6645161872&#45;&gt;6645158096 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>6645161872-&gt;6645158096</title>\n",
       "<path d=\"M673.959,-219.313C671.446,-211.202 668.389,-201.336 665.582,-192.277\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"668.864,-191.045 662.561,-182.529 662.178,-193.117 668.864,-191.045\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6341687440 -->\n",
       "<g class=\"node\" id=\"node19\"><title>6341687440</title>\n",
       "<polygon fill=\"none\" points=\"319.077,-73.5 319.077,-109.5 427.492,-109.5 427.492,-73.5 319.077,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.285\" y=\"-87.3\">merge_2: Merge</text>\n",
       "</g>\n",
       "<!-- 6572314384&#45;&gt;6341687440 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>6572314384-&gt;6341687440</title>\n",
       "<path d=\"M135.32,-150.56C184.141,-138.648 256.676,-120.951 308.822,-108.228\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"309.753,-111.604 318.639,-105.833 308.094,-104.803 309.753,-111.604\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6361963408&#45;&gt;6341687440 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>6361963408-&gt;6341687440</title>\n",
       "<path d=\"M268.339,-146.494C287.147,-136.817 310.899,-124.597 331.048,-114.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"332.731,-117.301 340.022,-109.614 329.528,-111.076 332.731,-117.301\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6576210064&#45;&gt;6341687440 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>6576210064-&gt;6341687440</title>\n",
       "<path d=\"M373.285,-146.313C373.285,-138.289 373.285,-128.547 373.285,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"376.785,-119.529 373.285,-109.529 369.785,-119.529 376.785,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6593489808&#45;&gt;6341687440 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>6593489808-&gt;6341687440</title>\n",
       "<path d=\"M478.23,-146.494C459.422,-136.817 435.67,-124.597 415.521,-114.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"417.041,-111.076 406.548,-109.614 413.839,-117.301 417.041,-111.076\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6645158096&#45;&gt;6341687440 -->\n",
       "<g class=\"edge\" id=\"edge22\"><title>6645158096-&gt;6341687440</title>\n",
       "<path d=\"M604.2,-150.229C556.939,-138.413 487.69,-121.101 437.36,-108.519\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"438.109,-105.098 427.559,-106.068 436.411,-111.889 438.109,-105.098\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6341824464 -->\n",
       "<g class=\"node\" id=\"node20\"><title>6341824464</title>\n",
       "<polygon fill=\"none\" points=\"321.159,-0.5 321.159,-36.5 425.411,-36.5 425.411,-0.5 321.159,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.285\" y=\"-14.3\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 6341687440&#45;&gt;6341824464 -->\n",
       "<g class=\"edge\" id=\"edge23\"><title>6341687440-&gt;6341824464</title>\n",
       "<path d=\"M373.285,-73.3129C373.285,-65.2895 373.285,-55.5475 373.285,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"376.785,-46.5288 373.285,-36.5288 369.785,-46.5289 376.785,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(VOCAB_SIZE, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length=EMBEDDING_INPUT_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(EMBEDDING_INPUT_LENGTH, ))\n",
    "x = embedding_layer(sequence_input)\n",
    "x = Flatten()(x)\n",
    "parts = []\n",
    "for i in range(MAX_SENTENCES):\n",
    "    y = Lambda(lambda x: x[:, i :: MAX_SENTENCES])(x) # Split by Num_sentences. We need an LSTM for each sentence\n",
    "    z = Reshape((MAX_WORD_LENGTH,EMBEDDING_DIM), input_shape=(MAX_WORD_LENGTH * EMBEDDING_DIM ,))(y)\n",
    "    l = LSTM(128)(z) # An LSTM for each sentence\n",
    "    parts.append(l)\n",
    "# Construct Document vector : 128 X MAX_SENTENCES\n",
    "merged = merge(parts, mode='concat', concat_axis= 1)\n",
    "# merged has the document representation  built from sentences \n",
    "\n",
    "preds = Dense(NUM_CLASSES, activation='softmax')(merged)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 690 samples, validate on 172 samples\n",
      "Epoch 1/5\n",
      "690/690 [==============================] - 405s - loss: 0.3860 - acc: 0.8348 - val_loss: 0.9978 - val_acc: 0.5872\n",
      "Epoch 2/5\n",
      "690/690 [==============================] - 403s - loss: 0.3719 - acc: 0.8420 - val_loss: 0.9924 - val_acc: 0.5640\n",
      "Epoch 3/5\n",
      "630/690 [==========================>...] - ETA: 35s - loss: 0.3347 - acc: 0.8762"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ba871f5c0af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/sivagurukannan/anaconda/envs/python2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sivagurukannan/anaconda/envs/python2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sivagurukannan/anaconda/envs/python2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sivagurukannan/anaconda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sivagurukannan/anaconda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sivagurukannan/anaconda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/sivagurukannan/anaconda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sivagurukannan/anaconda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,validation_data=(x_val, y_val),nb_epoch=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 10s    \n",
      "[0.86166602650354074, 0.59883720930232553]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Working Model \n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE, \n",
    "                            EMBEDDING_DIM, \n",
    "                            #weights = [embedding_matrix],\n",
    "                            input_length=EMBEDDING_INPUT_LENGTH,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "sequence_input = Input(shape=(EMBEDDING_INPUT_LENGTH, ))\n",
    "x = embedding_layer(sequence_input)\n",
    "x = Flatten()(x)\n",
    "parts = []\n",
    "for i in range(MAX_SENTENCES):\n",
    "    y = Lambda(lambda x: x[:, :: MAX_SENTENCES])(x) # Split by Num_sentences. We need an LSTM for each sentence\n",
    "    z = Reshape((MAX_WORD_LENGTH,EMBEDDING_DIM), input_shape=(MAX_WORD_LENGTH * EMBEDDING_DIM ,))(y)\n",
    "    l = LSTM(128)(z) # An LSTM for each sentence\n",
    "    parts.append(l)\n",
    "# Construct Document vector : 128 X MAX_SENTENCES\n",
    "merged = merge(parts, mode='concat', concat_axis= 1)\n",
    "# merged has the document representation  built from sentences \n",
    "x = Dense(128, activation='relu')(merged)\n",
    "preds = Dense(8, activation='softmax')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
