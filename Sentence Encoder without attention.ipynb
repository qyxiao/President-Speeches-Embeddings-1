{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sivagurukannan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "import chainer\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from chainer.training import extensions\n",
    "import os, sys, re\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string \n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 | packaged by conda-forge | (default, Jul 26 2016, 01:37:38) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.54)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = './speech_data/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the labels \n",
    "df = pd.read_csv('./speech_data/presidents_meta.csv')\n",
    "label_dict = dict(zip(list(df.foldername), list(df.label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_dict['arthur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 862 texts.\n",
      "Found 862 texts.\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "# labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if 'combined' not in fname and 'combines' not in fname:\n",
    "                president_name = path.split('/')[-1]\n",
    "                fpath = os.path.join(path, fname)\n",
    "                f = open(fpath)\n",
    "                if (president_name in label_dict):\n",
    "                    labels.append(label_dict[president_name])\n",
    "                    texts.append(f.read())\n",
    "                f.close()\n",
    "                \n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "print('Found %s texts.' % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE PRESIDENT. Good afternoon, ladies and gentlemen.I have been asked to give a statement about the consular convention that is pending before the United States Senate.I should like to say very briefly that I hope the Senate will give its advice and consent to the proposed convention with the U.S.S.R. I feel very strongly that the ratification of this treaty is very much in our national interest. I feel this way for two principal reasons:First, we need this treaty to protect 18,000 American citizens who each year travel from this country to the Soviet The convention requires immediate notification to us whenever an American is arrested in the Soviet Union. It insures our right to visit that citizen within 4 and as often thereafter as is desirable.We think that we need these rights help to protect American citizens. These are rights which the Soviet citizens already have who travel in this country, because guaranteed by our Constitution.Second, the convention does not require the openin'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[500][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Maximum words 522\n",
      "57\n",
      "Maximum words 659\n",
      "40\n",
      "Maximum words 772\n",
      "6\n",
      "Maximum words 1599\n",
      "398\n",
      "Maximum words 2364\n",
      "148\n",
      "Maximum words 3342\n",
      "135\n",
      "Maximum words 5273\n",
      "150\n",
      "Maximum words 5273\n",
      "24\n",
      "Maximum words 5273\n",
      "38\n",
      "Maximum words 5273\n",
      "97\n",
      "Maximum words 5273\n",
      "13\n",
      "Maximum words 5273\n",
      "10\n",
      "Maximum words 5273\n",
      "11\n",
      "Maximum words 5273\n",
      "21\n",
      "Maximum words 5273\n",
      "7\n",
      "Maximum words 5273\n",
      "153\n",
      "Maximum words 5273\n",
      "8\n",
      "Maximum words 5273\n",
      "8\n",
      "Maximum words 5273\n",
      "139\n",
      "Maximum words 5273\n",
      "299\n",
      "Maximum words 5273\n",
      "268\n",
      "Maximum words 5273\n",
      "343\n",
      "Maximum words 5273\n",
      "322\n",
      "Maximum words 5273\n",
      "253\n",
      "Maximum words 5273\n",
      "10\n",
      "Maximum words 5273\n",
      "18\n",
      "Maximum words 5273\n",
      "114\n",
      "Maximum words 5273\n",
      "21\n",
      "Maximum words 5273\n",
      "37\n",
      "Maximum words 5273\n",
      "72\n",
      "Maximum words 5273\n",
      "14\n",
      "Maximum words 5273\n",
      "205\n",
      "Maximum words 5273\n",
      "73\n",
      "Maximum words 5273\n",
      "34\n",
      "Maximum words 5273\n",
      "81\n",
      "Maximum words 5273\n",
      "310\n",
      "Maximum words 5273\n",
      "411\n",
      "Maximum words 5273\n",
      "332\n",
      "Maximum words 5273\n",
      "380\n",
      "Maximum words 5273\n",
      "124\n",
      "Maximum words 5273\n",
      "150\n",
      "Maximum words 5273\n",
      "290\n",
      "Maximum words 5273\n",
      "119\n",
      "Maximum words 5273\n",
      "40\n",
      "Maximum words 5273\n",
      "140\n",
      "Maximum words 5273\n",
      "66\n",
      "Maximum words 5273\n",
      "201\n",
      "Maximum words 5273\n",
      "121\n",
      "Maximum words 5273\n",
      "77\n",
      "Maximum words 5273\n",
      "86\n",
      "Maximum words 5273\n",
      "160\n",
      "Maximum words 5273\n",
      "177\n",
      "Maximum words 5273\n",
      "279\n",
      "Maximum words 5273\n",
      "187\n",
      "Maximum words 5273\n",
      "192\n",
      "Maximum words 5273\n",
      "65\n",
      "Maximum words 5273\n",
      "198\n",
      "Maximum words 5273\n",
      "824\n",
      "Maximum words 5273\n",
      "61\n",
      "Maximum words 5273\n",
      "35\n",
      "Maximum words 5273\n",
      "256\n",
      "Maximum words 5273\n",
      "1051\n",
      "Maximum words 5273\n",
      "475\n",
      "Maximum words 5273\n",
      "30\n",
      "Maximum words 5273\n",
      "175\n",
      "Maximum words 5273\n",
      "185\n",
      "Maximum words 5273\n",
      "160\n",
      "Maximum words 5273\n",
      "104\n",
      "Maximum words 5273\n",
      "121\n",
      "Maximum words 5273\n",
      "35\n",
      "Maximum words 5273\n",
      "203\n",
      "Maximum words 5273\n",
      "58\n",
      "Maximum words 5273\n",
      "107\n",
      "Maximum words 5273\n",
      "32\n",
      "Maximum words 5273\n",
      "37\n",
      "Maximum words 5273\n",
      "42\n",
      "Maximum words 5273\n",
      "92\n",
      "Maximum words 5273\n",
      "169\n",
      "Maximum words 5273\n",
      "635\n",
      "Maximum words 5273\n",
      "73\n",
      "Maximum words 5273\n",
      "142\n",
      "Maximum words 5273\n",
      "545\n",
      "Maximum words 5273\n",
      "529\n",
      "Maximum words 5273\n",
      "553\n",
      "Maximum words 5273\n",
      "115\n",
      "Maximum words 5273\n",
      "13\n",
      "Maximum words 5273\n",
      "5\n",
      "Maximum words 5273\n",
      "5\n",
      "Maximum words 5273\n",
      "11\n",
      "Maximum words 5273\n",
      "9\n",
      "Maximum words 5273\n",
      "41\n",
      "Maximum words 5273\n",
      "33\n",
      "Maximum words 5273\n",
      "471\n",
      "Maximum words 5273\n",
      "148\n",
      "Maximum words 5273\n",
      "71\n",
      "Maximum words 5273\n",
      "13\n",
      "Maximum words 5273\n",
      "9\n",
      "Maximum words 5273\n",
      "24\n",
      "Maximum words 5273\n",
      "31\n",
      "Maximum words 5273\n",
      "53\n",
      "Maximum words 5273\n",
      "9\n",
      "Maximum words 5273\n",
      "41\n",
      "Maximum words 5273\n",
      "54\n",
      "Maximum words 5273\n",
      "306\n",
      "Maximum words 5273\n",
      "208\n",
      "Maximum words 5273\n",
      "79\n",
      "Maximum words 5273\n",
      "179\n",
      "Maximum words 6573\n",
      "198\n",
      "Maximum words 6573\n",
      "262\n",
      "Maximum words 6573\n",
      "215\n",
      "Maximum words 6573\n",
      "262\n",
      "Maximum words 6573\n",
      "79\n",
      "Maximum words 6573\n",
      "52\n",
      "Maximum words 6573\n",
      "22\n",
      "Maximum words 6573\n",
      "103\n",
      "Maximum words 6573\n",
      "16\n",
      "Maximum words 6573\n",
      "92\n",
      "Maximum words 6573\n",
      "292\n",
      "Maximum words 6573\n",
      "212\n",
      "Maximum words 6573\n",
      "397\n",
      "Maximum words 6573\n",
      "69\n",
      "Maximum words 6573\n",
      "35\n",
      "Maximum words 6573\n",
      "368\n",
      "Maximum words 6573\n",
      "53\n",
      "Maximum words 6573\n",
      "152\n",
      "Maximum words 6573\n",
      "110\n",
      "Maximum words 6573\n",
      "370\n",
      "Maximum words 6573\n",
      "54\n",
      "Maximum words 6573\n",
      "357\n",
      "Maximum words 6573\n",
      "121\n",
      "Maximum words 6573\n",
      "152\n",
      "Maximum words 6573\n",
      "52\n",
      "Maximum words 6573\n",
      "26\n",
      "Maximum words 6573\n",
      "95\n",
      "Maximum words 6573\n",
      "75\n",
      "Maximum words 6573\n",
      "54\n",
      "Maximum words 6573\n",
      "114\n",
      "Maximum words 6573\n",
      "36\n",
      "Maximum words 6573\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bd046f3861e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Maximum words {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_NB_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMAX_NB_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mMAX_NB_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_max_nb_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-bd046f3861e8>\u001b[0m in \u001b[0;36mfind_max_nb_words\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mlocal_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m   \u001b[0mlocal_max\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mMAX_NB_WORDS\u001b[0m  \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mMAX_NB_WORDS\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlocal_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def find_max_nb_words(texts):\n",
    "    MAX_NB_WORDS = 0\n",
    "    for text in texts:\n",
    "        sents = tokenize.sent_tokenize(text)\n",
    "        print(len(sents))\n",
    "        local_max = max(list(map(len, sents)))\n",
    "        if   local_max > MAX_NB_WORDS  :\n",
    "            MAX_NB_WORDS =  local_max\n",
    "        print(\"Maximum words {}\".format(MAX_NB_WORDS))\n",
    "    return MAX_NB_WORDS\n",
    "MAX_NB_WORDS = find_max_nb_words(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_NB_WORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-acaa9847a714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remove extra and trailing spaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mnorm_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_NB_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_NB_WORDS' is not defined"
     ]
    }
   ],
   "source": [
    "def normalize_corpus(text, replace_period=False):\n",
    "    import string \n",
    "    sents = tokenize.sent_tokenize(text)\n",
    "    sents = map(lambda x: x.replace('.',' '), sents)\n",
    "    local_max = max(map(len, sents))\n",
    "    if   local_max > MAX_NB_WORDS:\n",
    "        MAX_NB_WORDS =  max(map(len, sents))\n",
    "    print(\"Maximum words {}\".format(MAX_NB_WORDS))\n",
    "    text = ' . '.join(sents)      \n",
    "    for char in string.punctuation:\n",
    "        if not replace_period and char=='.':\n",
    "            text = text.replace(char, ' . ') #text.replace(char, ' ' + char + ' ')\n",
    "        else:\n",
    "            text = text.replace(char, ' ') #text.replace(char, ' ' + char + ' ')\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text) #remove non-ASCII chars\n",
    "    text = re.sub( '\\s+', ' ', text).lstrip().rstrip() #remove extra and trailing spaces\n",
    "    return text.lower()\n",
    "norm_texts = list(map(normalize_corpus, texts, MAX_NB_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' '.join(tokenize.sent_tokenize(texts[500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_corpus(texts[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2665, 11080, 10013, 1927, 81934, 62451, 51770, 54751, 5014, 6845]\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 100\n",
    "len(texts)\n",
    "mini_test = texts[:10]\n",
    "print(list(map(len, mini_test)))\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_SENTENCES = 100\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_dense_list(speech, maxsentence):\n",
    "    \n",
    "    each_sub_len = len(speech[0])\n",
    "    Z = np.zeros((maxsentence, each_sub_len))\n",
    "    #print(\"Shape of Z{}, list {}\", Z.shape, len(speech))\n",
    "    for index, row in enumerate(speech):\n",
    "        if (index >= maxsentence):\n",
    "            break\n",
    "        \n",
    "        Z[index] = row\n",
    "    #print(Z) \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_text(mini_test):\n",
    "    #mini_test = \"mini test. hello world.\"\n",
    "    tokenizer = Tokenizer(nb_words=5)\n",
    "    tokenizer.fit_on_texts(mini_test)\n",
    "    text_encoded = np.zeros((len(texts),MAX_SENTENCES,MAX_SEQUENCE_LENGTH))\n",
    "    #print(text_encoded.shape)\n",
    "    word_index = tokenizer.word_index\n",
    "    for enu, speech in enumerate(mini_test):\n",
    "        sents = tokenize.sent_tokenize(speech)\n",
    "        if(len(sents) == 0):\n",
    "            continue\n",
    "        sequences = tokenizer.texts_to_sequences(sents)\n",
    "        \n",
    "        #print(len(sequences))\n",
    "        #print('Found %s unique tokens.' % len(word_index))\n",
    "        data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        text_encoded[enu] = (pad_dense_list(data, 100))\n",
    "    return word_index, text_encoded\n",
    "word_index, text_encoded = preprocess_text(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(862, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "# First Speech encoding\n",
    "data = text_encoded\n",
    "labels = np.asarray(labels)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = data\n",
    "\n",
    "data = data.reshape(data.shape[0], data.shape[1]*data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data = np.zeros((800, 100))\n",
    "labels = np.zeros(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (800, 100)\n",
      "Shape of label tensor: (800,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "# labels = labels[indices]\n",
    "VALIDATION_SPLIT = 0.2\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "GLOVE_DIR = '../glove.6B/'\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=data.shape[1],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/2\n",
      "100/640 [===>..........................] - ETA: 30s - loss: 1.1200 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(data.shape[1],), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = LSTM(100)(embedded_sequences)\n",
    "preds = Dense(7, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=2, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Experiment with sequence classification \n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train_imdb, y_train_imdb), (X_test_imdb, y_test_imdb)  = (X_train, y_train), (X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_train_imdb[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
